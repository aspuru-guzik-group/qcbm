{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamad/miniconda3/envs/qcbm-ibmq/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `molecular_data` is a list of molecular structures in SELFIES format\n",
    "molecular_data = [\n",
    "    'C[C@H](N)C(=O)O',       # Alanine\n",
    "    'CC(C)C[C@H](N)C(=O)O',  # Leucine\n",
    "    'N[C@@H](CC1=CNC2=C1C=CC=C2)C(=O)O',  # Tryptophan\n",
    "    'CC(C)(C)C(=O)N[C@@H](CCC(=O)O)C(=O)O',  # Valine\n",
    "    'CC(C)CC[C@@H](C(=O)O)N',  # Isoleucine\n",
    "    'CC1=CC(=CC=C1)C[C@H](N)C(=O)O',  # Phenylalanine\n",
    "    'CC(C)CC(=O)O',           # Butanoic acid\n",
    "    'C1CCC(CC1)NC(=O)C2=CC=CC=C2',  # Cyclohexylphenylurea\n",
    "    'CC(=O)OC1=CC=CC=C1C(=O)O',  # Aspirin\n",
    "    'CCO',  # Ethanol\n",
    "]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenized_data = [tokenizer.encode(molecule, return_tensors='pt') for molecule in molecular_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(vocab_size=tokenizer.vocab_size, n_positions=512, n_ctx=512, n_embd=768, n_layer=12, n_head=12)\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tokenized_data:\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 0.09984041750431061\n",
      "Evaluation loss: 0.29020264744758606\n",
      "Evaluation loss: 0.04059005528688431\n",
      "Evaluation loss: 0.19716304540634155\n",
      "Evaluation loss: 0.30552205443382263\n",
      "Evaluation loss: 0.20819862186908722\n",
      "Evaluation loss: 0.2720796763896942\n",
      "Evaluation loss: 0.0556388720870018\n",
      "Evaluation loss: 0.18612971901893616\n",
      "Evaluation loss: 0.32996782660484314\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tokenized_data:\n",
    "        outputs = model(batch, labels=batch)\n",
    "        eval_loss = outputs.loss.item()\n",
    "        print(f'Evaluation loss: {eval_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Molecule: [START]CC1NC2=CNC2=C1C=CC=C2)C(=O)O)O)O)O)O)O)O)O)N[C(=O)\n"
     ]
    }
   ],
   "source": [
    "def generate_molecule(prompt, max_length=50, temperature=1.0):\n",
    "    # Handle empty prompt case\n",
    "    if not prompt:\n",
    "        prompt = \"[START]\"  # Use a special token or any non-empty string\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_molecule = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_molecule\n",
    "\n",
    "# Example prompt (can be a partial SELFIES string or non-empty string)\n",
    "prompt = \"[START]\"\n",
    "new_molecule = generate_molecule(prompt)\n",
    "print(\"Generated Molecule:\", new_molecule)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qcbm-ibmq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
