{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamad/miniconda3/envs/qcbm-ibmq/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `molecular_data` is a list of molecular structures in SELFIES format\n",
    "molecular_data = [\n",
    "    'C[C@H](N)C(=O)O',       # Alanine\n",
    "    'CC(C)C[C@H](N)C(=O)O',  # Leucine\n",
    "    'N[C@@H](CC1=CNC2=C1C=CC=C2)C(=O)O',  # Tryptophan\n",
    "    'CC(C)(C)C(=O)N[C@@H](CCC(=O)O)C(=O)O',  # Valine\n",
    "    'CC(C)CC[C@@H](C(=O)O)N',  # Isoleucine\n",
    "    'CC1=CC(=CC=C1)C[C@H](N)C(=O)O',  # Phenylalanine\n",
    "    'CC(C)CC(=O)O',           # Butanoic acid\n",
    "    'C1CCC(CC1)NC(=O)C2=CC=CC=C2',  # Cyclohexylphenylurea\n",
    "    'CC(=O)OC1=CC=CC=C1C(=O)O',  # Aspirin\n",
    "    'CCO',  # Ethanol\n",
    "]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenized_data = [tokenizer.encode(molecule, return_tensors='pt') for molecule in molecular_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(vocab_size=tokenizer.vocab_size, n_positions=512, n_ctx=512, n_embd=768, n_layer=12, n_head=12)\n",
    "model = GPT2LMHeadModel(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tokenized_data:\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation loss: 0.09984041750431061\n",
      "Evaluation loss: 0.29020264744758606\n",
      "Evaluation loss: 0.04059005528688431\n",
      "Evaluation loss: 0.19716304540634155\n",
      "Evaluation loss: 0.30552205443382263\n",
      "Evaluation loss: 0.20819862186908722\n",
      "Evaluation loss: 0.2720796763896942\n",
      "Evaluation loss: 0.0556388720870018\n",
      "Evaluation loss: 0.18612971901893616\n",
      "Evaluation loss: 0.32996782660484314\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tokenized_data:\n",
    "        outputs = model(batch, labels=batch)\n",
    "        eval_loss = outputs.loss.item()\n",
    "        print(f'Evaluation loss: {eval_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Molecule: [START]CC1NC2=CNC2=C1C=CC=C2)C(=O)O)O)O)O)O)O)O)O)N[C(=O)\n"
     ]
    }
   ],
   "source": [
    "def generate_molecule(prompt, max_length=50, temperature=1.0):\n",
    "    # Handle empty prompt case\n",
    "    if not prompt:\n",
    "        prompt = \"[START]\"  # Use a special token or any non-empty string\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_molecule = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_molecule\n",
    "\n",
    "# Example prompt (can be a partial SELFIES string or non-empty string)\n",
    "prompt = \"[START]\"\n",
    "new_molecule = generate_molecule(prompt)\n",
    "print(\"Generated Molecule:\", new_molecule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from qiskit import QuantumCircuit, transpile, assemble\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit.visualization import plot_histogram\n",
    "import selfies as sf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the quantum simulator\n",
    "simulator = AerSimulator()\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example mapping from SELFIES tokens to quantum gates\n",
    "token_to_gate = {\n",
    "    'C': 'h',  # Hadamard gate\n",
    "    'O': 'x',  # Pauli-X gate\n",
    "    'N': 'z',  # Pauli-Z gate\n",
    "    '[=O]': 'y',  # Pauli-Y gate\n",
    "    '[C@H]': 'rx',  # Rotation around X-axis\n",
    "    '[C@@H]': 'ry',  # Rotation around Y-axis\n",
    "    '[C@@]': 'rz',  # Rotation around Z-axis\n",
    "    '(': 'cx',  # CNOT gate (control X)\n",
    "    ')': 'cz',  # CNOT gate (control Z)\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Function to add gates to the quantum circuit based on token\n",
    "def add_gate(qc, token, qubit):\n",
    "    if token in token_to_gate:\n",
    "        gate = token_to_gate[token]\n",
    "        if gate == 'h':\n",
    "            qc.h(qubit)\n",
    "        elif gate == 'x':\n",
    "            qc.x(qubit)\n",
    "        elif gate == 'z':\n",
    "            qc.z(qubit)\n",
    "        elif gate == 'y':\n",
    "            qc.y(qubit)\n",
    "        elif gate == 'rx':\n",
    "            qc.rx(np.pi/2, qubit)\n",
    "        elif gate == 'ry':\n",
    "            qc.ry(np.pi/2, qubit)\n",
    "        elif gate == 'rz':\n",
    "            qc.rz(np.pi/2, qubit)\n",
    "        elif gate == 'cx':\n",
    "            qc.cx(qubit, (qubit + 1) % qc.num_qubits)\n",
    "        elif gate == 'cz':\n",
    "            qc.cz(qubit, (qubit + 1) % qc.num_qubits)\n",
    "        # Add more gates as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quantum_circuit(tokens):\n",
    "    tokens = list(tokens)  # Convert generator to list\n",
    "    num_qubits = len(tokens)\n",
    "    qc = QuantumCircuit(num_qubits, num_qubits)\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        add_gate(qc, token, i)\n",
    "    \n",
    "    # Measure the qubits\n",
    "    qc.measure(range(num_qubits), range(num_qubits))\n",
    "    \n",
    "    return qc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_circuit(qc):\n",
    "    compiled_circuit = transpile(qc, simulator)\n",
    "    qobj = assemble(compiled_circuit)\n",
    "    result = simulator.run(qobj).result()\n",
    "    counts = result.get_counts(qc)\n",
    "    return counts\n",
    "\n",
    "def decode_output(counts):\n",
    "    # Example decoding logic (needs to be customized based on your encoding)\n",
    "    most_common_state = max(counts, key=counts.get)\n",
    "    decoded_selfies = \"\"\n",
    "    for bit in most_common_state:\n",
    "        decoded_selfies += \"C\" if bit == \"0\" else \"O\"  # Example decoding\n",
    "    return decoded_selfies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malformed SELFIES string: [START]\n",
      "\n",
      "HISTORY. A new statute, by a combination of clauses, will soon be made of the most ancient or most solemn articles: namely: A new statute, by a combination of clauses, will soon be made of the\n",
      "Generated Molecule: None\n"
     ]
    }
   ],
   "source": [
    "def generate_molecule(prompt, max_length=50, temperature=1.0):\n",
    "    if not prompt:\n",
    "        prompt = \"[START]\"  # Use a special token or any non-empty string\n",
    "\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate tokens\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95\n",
    "        )\n",
    "    \n",
    "    # Decode tokens to string\n",
    "    generated_selfies = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Validate the generated SELFIES string\n",
    "    try:\n",
    "        tokens = list(sf.split_selfies(generated_selfies))  # Convert generator to list\n",
    "    except ValueError as e:\n",
    "        print(f\"Malformed SELFIES string: {generated_selfies}\")\n",
    "        return None  # Or handle the malformed SELFIES appropriately\n",
    "\n",
    "    # Generate quantum circuit\n",
    "    print(tokens)\n",
    "    qc = generate_quantum_circuit(tokens)\n",
    "    \n",
    "    # Simulate quantum circuit\n",
    "    counts = simulate_circuit(qc)\n",
    "    \n",
    "    # Decode the output to SELFIES\n",
    "    decoded_selfies = decode_output(counts)\n",
    "    \n",
    "    return decoded_selfies\n",
    "\n",
    "# Example prompt (can be a partial SELFIES string or non-empty string)\n",
    "prompt = \"[START]\"\n",
    "new_molecule = generate_molecule(prompt)\n",
    "print(\"Generated Molecule:\", new_molecule)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qcbm-ibmq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
